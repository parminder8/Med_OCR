{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87cf4ec5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lmdb'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-855118f079cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mdataloader_iam\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataLoaderIAM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDecoderType\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPreprocessor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\SimpleHTR-master\\src\\dataloader_iam.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mlmdb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'lmdb'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "from typing import Tuple, List\n",
    "\n",
    "import cv2\n",
    "import editdistance\n",
    "from path import Path\n",
    "\n",
    "from dataloader_iam import DataLoaderIAM, Batch\n",
    "from model import Model, DecoderType\n",
    "from preprocessor import Preprocessor\n",
    "\n",
    "\n",
    "class FilePaths:\n",
    "    \"\"\"Filenames and paths to data.\"\"\"\n",
    "    fn_char_list = '../model/charList.txt'\n",
    "    fn_summary = '../model/summary.json'\n",
    "    fn_corpus = '../data/corpus.txt'\n",
    "\n",
    "\n",
    "def get_img_height() -> int:\n",
    "    \"\"\"Fixed height for NN.\"\"\"\n",
    "    return 32\n",
    "\n",
    "\n",
    "def get_img_size(line_mode: bool = False) -> Tuple[int, int]:\n",
    "    \"\"\"Height is fixed for NN, width is set according to training mode (single words or text lines).\"\"\"\n",
    "    if line_mode:\n",
    "        return 256, get_img_height()\n",
    "    return 128, get_img_height()\n",
    "\n",
    "\n",
    "def write_summary(char_error_rates: List[float], word_accuracies: List[float]) -> None:\n",
    "    \"\"\"Writes training summary file for NN.\"\"\"\n",
    "    with open(FilePaths.fn_summary, 'w') as f:\n",
    "        json.dump({'charErrorRates': char_error_rates, 'wordAccuracies': word_accuracies}, f)\n",
    "\n",
    "\n",
    "def train(model: Model,\n",
    "          loader: DataLoaderIAM,\n",
    "          line_mode: bool,\n",
    "          early_stopping: int = 25) -> None:\n",
    "    \"\"\"Trains NN.\"\"\"\n",
    "    epoch = 0  # number of training epochs since start\n",
    "    summary_char_error_rates = []\n",
    "    summary_word_accuracies = []\n",
    "    preprocessor = Preprocessor(get_img_size(line_mode), data_augmentation=True, line_mode=line_mode)\n",
    "    best_char_error_rate = float('inf')  # best valdiation character error rate\n",
    "    no_improvement_since = 0  # number of epochs no improvement of character error rate occurred\n",
    "    # stop training after this number of epochs without improvement\n",
    "    while True:\n",
    "        epoch += 1\n",
    "        print('Epoch:', epoch)\n",
    "\n",
    "        # train\n",
    "        print('Train NN')\n",
    "        loader.train_set()\n",
    "        while loader.has_next():\n",
    "            iter_info = loader.get_iterator_info()\n",
    "            batch = loader.get_next()\n",
    "            batch = preprocessor.process_batch(batch)\n",
    "            loss = model.train_batch(batch)\n",
    "            print(f'Epoch: {epoch} Batch: {iter_info[0]}/{iter_info[1]} Loss: {loss}')\n",
    "\n",
    "        # validate\n",
    "        char_error_rate, word_accuracy = validate(model, loader, line_mode)\n",
    "\n",
    "        # write summary\n",
    "        summary_char_error_rates.append(char_error_rate)\n",
    "        summary_word_accuracies.append(word_accuracy)\n",
    "        write_summary(summary_char_error_rates, summary_word_accuracies)\n",
    "\n",
    "        # if best validation accuracy so far, save model parameters\n",
    "        if char_error_rate < best_char_error_rate:\n",
    "            print('Character error rate improved, save model')\n",
    "            best_char_error_rate = char_error_rate\n",
    "            no_improvement_since = 0\n",
    "            model.save()\n",
    "        else:\n",
    "            print(f'Character error rate not improved, best so far: {char_error_rate * 100.0}%')\n",
    "            no_improvement_since += 1\n",
    "\n",
    "        # stop training if no more improvement in the last x epochs\n",
    "        if no_improvement_since >= early_stopping:\n",
    "            print(f'No more improvement since {early_stopping} epochs. Training stopped.')\n",
    "            break\n",
    "\n",
    "\n",
    "def validate(model: Model, loader: DataLoaderIAM, line_mode: bool) -> Tuple[float, float]:\n",
    "    \"\"\"Validates NN.\"\"\"\n",
    "    print('Validate NN')\n",
    "    loader.validation_set()\n",
    "    preprocessor = Preprocessor(get_img_size(line_mode), line_mode=line_mode)\n",
    "    num_char_err = 0\n",
    "    num_char_total = 0\n",
    "    num_word_ok = 0\n",
    "    num_word_total = 0\n",
    "    while loader.has_next():\n",
    "        iter_info = loader.get_iterator_info()\n",
    "        print(f'Batch: {iter_info[0]} / {iter_info[1]}')\n",
    "        batch = loader.get_next()\n",
    "        batch = preprocessor.process_batch(batch)\n",
    "        recognized, _ = model.infer_batch(batch)\n",
    "\n",
    "        print('Ground truth -> Recognized')\n",
    "        for i in range(len(recognized)):\n",
    "            num_word_ok += 1 if batch.gt_texts[i] == recognized[i] else 0\n",
    "            num_word_total += 1\n",
    "            dist = editdistance.eval(recognized[i], batch.gt_texts[i])\n",
    "            num_char_err += dist\n",
    "            num_char_total += len(batch.gt_texts[i])\n",
    "            print('[OK]' if dist == 0 else '[ERR:%d]' % dist, '\"' + batch.gt_texts[i] + '\"', '->',\n",
    "                  '\"' + recognized[i] + '\"')\n",
    "\n",
    "    # print validation result\n",
    "    char_error_rate = num_char_err / num_char_total\n",
    "    word_accuracy = num_word_ok / num_word_total\n",
    "    print(f'Character error rate: {char_error_rate * 100.0}%. Word accuracy: {word_accuracy * 100.0}%.')\n",
    "    return char_error_rate, word_accuracy\n",
    "\n",
    "\n",
    "def infer(model: Model, fn_img: Path) -> None:\n",
    "    \"\"\"Recognizes text in image provided by file path.\"\"\"\n",
    "    img = cv2.imread(fn_img, cv2.IMREAD_GRAYSCALE)\n",
    "    assert img is not None\n",
    "\n",
    "    preprocessor = Preprocessor(get_img_size(), dynamic_width=True, padding=16)\n",
    "    img = preprocessor.process_img(img)\n",
    "\n",
    "    batch = Batch([img], None, 1)\n",
    "    recognized, probability = model.infer_batch(batch, True)\n",
    "    print(f'Recognized: \"{recognized[0]}\"')\n",
    "    print(f'Probability: {probability[0]}')\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function.\"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--mode', choices=['train', 'validate', 'infer'], default='infer')\n",
    "    parser.add_argument('--decoder', choices=['bestpath', 'beamsearch', 'wordbeamsearch'], default='bestpath')\n",
    "    parser.add_argument('--batch_size', help='Batch size.', type=int, default=100)\n",
    "    parser.add_argument('--data_dir', help='Directory containing IAM dataset.', type=Path, required=False)\n",
    "    parser.add_argument('--fast', help='Load samples from LMDB.', action='store_true')\n",
    "    parser.add_argument('--line_mode', help='Train to read text lines instead of single words.', action='store_true')\n",
    "    parser.add_argument('--img_file', help='Image used for inference.', type=Path, default='../data/word.png')\n",
    "    parser.add_argument('--early_stopping', help='Early stopping epochs.', type=int, default=25)\n",
    "    parser.add_argument('--dump', help='Dump output of NN to CSV file(s).', action='store_true')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # set chosen CTC decoder\n",
    "    decoder_mapping = {'bestpath': DecoderType.BestPath,\n",
    "                       'beamsearch': DecoderType.BeamSearch,\n",
    "                       'wordbeamsearch': DecoderType.WordBeamSearch}\n",
    "    decoder_type = decoder_mapping[args.decoder]\n",
    "\n",
    "    # train or validate on IAM dataset\n",
    "    if args.mode in ['train', 'validate']:\n",
    "        # load training data, create TF model\n",
    "        loader = DataLoaderIAM(args.data_dir, args.batch_size, fast=args.fast)\n",
    "        char_list = loader.char_list\n",
    "\n",
    "        # when in line mode, take care to have a whitespace in the char list\n",
    "        if args.line_mode and ' ' not in char_list:\n",
    "            char_list = [' '] + char_list\n",
    "\n",
    "        # save characters of model for inference mode\n",
    "        open(FilePaths.fn_char_list, 'w').write(''.join(char_list))\n",
    "\n",
    "        # save words contained in dataset into file\n",
    "        open(FilePaths.fn_corpus, 'w').write(' '.join(loader.train_words + loader.validation_words))\n",
    "\n",
    "        # execute training or validation\n",
    "        if args.mode == 'train':\n",
    "            model = Model(char_list, decoder_type)\n",
    "            train(model, loader, line_mode=args.line_mode, early_stopping=args.early_stopping)\n",
    "        elif args.mode == 'validate':\n",
    "            model = Model(char_list, decoder_type, must_restore=True)\n",
    "            validate(model, loader, args.line_mode)\n",
    "\n",
    "    # infer text on test image\n",
    "    elif args.mode == 'infer':\n",
    "        model = Model(list(open(FilePaths.fn_char_list).read()), decoder_type, must_restore=True, dump=args.dump)\n",
    "        infer(model, args.img_file)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
